"""
Enterprise RAG Platform - Integration & End-to-End Validation Suite
Validates complete workflows and service integration across the entire platform.
"""

import asyncio
import json
import time
import tempfile
import base64
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
import sys
import os
from pathlib import Path

# Add project root to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import httpx
from io import BytesIO


@dataclass
class IntegrationTestResult:
    """Integration test result."""
    test_name: str
    workflow: str  # e2e_rag, document_processing, etc.
    passed: bool
    execution_time_ms: float = 0.0
    steps_completed: int = 0
    total_steps: int = 0
    description: str = ""
    error_message: str = ""
    workflow_data: Dict[str, Any] = None
    performance_metrics: Dict[str, float] = None


class IntegrationValidator:
    """Comprehensive integration and end-to-end validation suite."""
    
    def __init__(self):
        self.results: List[IntegrationTestResult] = []
        self.http_client = httpx.AsyncClient(timeout=60.0)  # Longer timeout for E2E tests
        
        # Service endpoints
        self.endpoints = {
            "api_gateway": "http://localhost:8000",
            "document_processor": "http://localhost:8001",
            "query_intelligence": "http://localhost:8002",
            "vector_search": "http://localhost:8003",
            "observability": "http://localhost:8004"
        }
        
        # Test data
        self.test_documents = {
            "simple_text": self._create_test_text_document(),
            "pdf_with_tables": self._create_test_pdf_bytes(),
            "json_data": self._create_test_json_document()
        }
        
        # Test queries
        self.test_queries = [
            "What is the main topic of the document?",
            "List all the key findings mentioned",
            "What are the financial metrics in the table?",
            "Summarize the conclusions"
        ]
    
    async def run_comprehensive_integration_validation(self) -> Dict[str, Any]:
        """Run comprehensive integration validation suite."""
        print("ğŸ”„ Starting Integration & End-to-End Validation Suite")
        print("=" * 60)
        
        start_time = time.time()
        
        # 1. Service Integration Testing
        print("\nğŸ”— Testing Service Integration...")
        await self._test_service_integration()
        
        # 2. Complete RAG Pipeline Testing
        print("\nğŸ“„ Testing Complete RAG Pipeline...")
        await self._test_complete_rag_pipeline()
        
        # 3. Multi-Modal Document Processing
        print("\nğŸ–¼ï¸ Testing Multi-Modal Document Processing...")
        await self._test_multimodal_processing()
        
        # 4. Query Intelligence Workflow
        print("\nğŸ§  Testing Query Intelligence Workflow...")
        await self._test_query_intelligence_workflow()
        
        # 5. Vector Search Integration
        print("\nğŸ” Testing Vector Search Integration...")
        await self._test_vector_search_integration()
        
        # 6. Cross-Service Data Flow
        print("\nğŸŒŠ Testing Cross-Service Data Flow...")
        await self._test_cross_service_data_flow()
        
        # 7. Error Handling & Recovery
        print("\nğŸ›¡ï¸ Testing Error Handling & Recovery...")
        await self._test_error_handling_recovery()
        
        # 8. Performance Under Load
        print("\nâš¡ Testing Performance Under Load...")
        await self._test_performance_under_load()
        
        # 9. Data Consistency Testing
        print("\nğŸ“Š Testing Data Consistency...")
        await self._test_data_consistency()
        
        # 10. User Experience Workflows
        print("\nğŸ‘¤ Testing User Experience Workflows...")
        await self._test_user_experience_workflows()
        
        total_time = time.time() - start_time
        
        return self._generate_integration_report(total_time)
    
    async def _test_service_integration(self):
        """Test integration between all services."""
        
        # Test 1: Service discovery and communication
        await self._test_service_discovery_integration()
        
        # Test 2: Health check propagation
        await self._test_health_check_integration()
        
        # Test 3: Circuit breaker integration
        await self._test_circuit_breaker_integration()
    
    async def _test_complete_rag_pipeline(self):
        """Test complete RAG pipeline end-to-end."""
        
        pipeline_steps = [
            "Document upload",
            "Document processing",
            "Embedding generation",
            "Vector storage",
            "Query processing",
            "Vector search",
            "Response generation"
        ]
        
        start_time = time.time()
        completed_steps = 0
        workflow_data = {}
        
        try:
            # Step 1: Document upload
            print("   Step 1/7: Document upload...")
            document_data = {
                "content": self.test_documents["simple_text"],
                "filename": "test_document.txt",
                "content_type": "text/plain"
            }
            
            upload_response = await self.http_client.post(
                f"{self.endpoints['api_gateway']}/api/v1/documents/process",
                json=document_data
            )
            
            if upload_response.status_code in [200, 201, 202]:
                completed_steps += 1
                workflow_data["document_uploaded"] = True
                print(f"     âœ… Document upload successful ({upload_response.status_code})")
            else:
                print(f"     âŒ Document upload failed ({upload_response.status_code})")
            
            # Small delay for processing
            await asyncio.sleep(2)
            
            # Step 2: Query the uploaded document
            print("   Step 2/7: Query processing...")
            query_data = {"query_text": self.test_queries[0]}
            
            query_response = await self.http_client.post(
                f"{self.endpoints['api_gateway']}/api/v1/query/analyze",
                json=query_data
            )
            
            if query_response.status_code in [200, 202]:
                completed_steps += 1
                workflow_data["query_processed"] = True
                print(f"     âœ… Query processing successful ({query_response.status_code})")
            else:
                print(f"     âŒ Query processing failed ({query_response.status_code})")
            
            # Step 3: Vector search
            print("   Step 3/7: Vector search...")
            search_data = {"query": self.test_queries[0], "k": 5}
            
            search_response = await self.http_client.post(
                f"{self.endpoints['api_gateway']}/api/v1/search",
                json=search_data
            )
            
            if search_response.status_code in [200, 202]:
                completed_steps += 1
                workflow_data["search_executed"] = True
                print(f"     âœ… Vector search successful ({search_response.status_code})")
            else:
                print(f"     âŒ Vector search failed ({search_response.status_code})")
            
            # Step 4: Complete RAG query
            print("   Step 4/7: Complete RAG query...")
            rag_data = {"query": self.test_queries[0]}
            
            rag_response = await self.http_client.post(
                f"{self.endpoints['api_gateway']}/api/v1/rag/complete",
                json=rag_data
            )
            
            if rag_response.status_code in [200, 202]:
                completed_steps += 1
                workflow_data["rag_completed"] = True
                print(f"     âœ… Complete RAG query successful ({rag_response.status_code})")
                
                # Check if response contains meaningful content
                try:
                    response_data = rag_response.json()
                    if response_data and len(str(response_data)) > 10:
                        completed_steps += 1
                        workflow_data["response_generated"] = True
                        print("     âœ… Response generation successful")
                except:
                    print("     âš ï¸ Response format validation failed")
            else:
                print(f"     âŒ Complete RAG query failed ({rag_response.status_code})")
            
            # Additional workflow validation steps
            completed_steps = min(completed_steps, len(pipeline_steps))
            
        except Exception as e:
            workflow_data["error"] = str(e)
            print(f"     âŒ Pipeline error: {str(e)}")
        
        execution_time = (time.time() - start_time) * 1000
        pipeline_success = completed_steps >= len(pipeline_steps) * 0.6  # 60% success rate
        
        self.results.append(IntegrationTestResult(
            test_name="Complete RAG Pipeline",
            workflow="e2e_rag",
            passed=pipeline_success,
            execution_time_ms=execution_time,
            steps_completed=completed_steps,
            total_steps=len(pipeline_steps),
            description=f"End-to-end RAG pipeline {'successful' if pipeline_success else 'partially failed'} ({completed_steps}/{len(pipeline_steps)} steps)",
            workflow_data=workflow_data,
            performance_metrics={"total_time_ms": execution_time, "steps_per_second": completed_steps / (execution_time / 1000)}
        ))
    
    async def _test_multimodal_processing(self):
        """Test multi-modal document processing."""
        
        processing_steps = [
            "PDF upload",
            "Text extraction", 
            "Table extraction",
            "Image analysis",
            "Multi-modal indexing"
        ]
        
        start_time = time.time()
        completed_steps = 0
        workflow_data = {}
        
        try:
            # Test PDF processing with tables and images
            print("   Testing PDF with tables and images...")
            
            pdf_data = {
                "content": base64.b64encode(self.test_documents["pdf_with_tables"]).decode(),
                "filename": "test_document.pdf",
                "content_type": "application/pdf",
                "enable_multimodal": True
            }
            
            response = await self.http_client.post(
                f"{self.endpoints['document_processor']}/api/v1/process",
                json=pdf_data
            )
            
            if response.status_code in [200, 202]:
                completed_steps += 1
                workflow_data["pdf_uploaded"] = True
                
                try:
                    result_data = response.json()
                    
                    # Check for text extraction
                    if "text" in str(result_data).lower():
                        completed_steps += 1
                        workflow_data["text_extracted"] = True
                        print("     âœ… Text extraction successful")
                    
                    # Check for table extraction
                    if "table" in str(result_data).lower():
                        completed_steps += 1
                        workflow_data["tables_extracted"] = True
                        print("     âœ… Table extraction successful")
                    
                    # Check for image processing
                    if "image" in str(result_data).lower():
                        completed_steps += 1
                        workflow_data["images_processed"] = True
                        print("     âœ… Image processing successful")
                    
                    # Check for multi-modal indexing
                    if "indexed" in str(result_data).lower() or completed_steps >= 3:
                        completed_steps += 1
                        workflow_data["multimodal_indexed"] = True
                        print("     âœ… Multi-modal indexing successful")
                        
                except json.JSONDecodeError:
                    print("     âš ï¸ Response format validation failed")
            else:
                print(f"     âŒ PDF processing failed ({response.status_code})")
                
        except Exception as e:
            workflow_data["error"] = str(e)
            print(f"     âŒ Multi-modal processing error: {str(e)}")
        
        execution_time = (time.time() - start_time) * 1000
        multimodal_success = completed_steps >= len(processing_steps) * 0.6
        
        self.results.append(IntegrationTestResult(
            test_name="Multi-Modal Document Processing",
            workflow="multimodal_processing",
            passed=multimodal_success,
            execution_time_ms=execution_time,
            steps_completed=completed_steps,
            total_steps=len(processing_steps),
            description=f"Multi-modal processing {'successful' if multimodal_success else 'partially failed'} ({completed_steps}/{len(processing_steps)} capabilities)",
            workflow_data=workflow_data
        ))
    
    async def _test_query_intelligence_workflow(self):
        """Test query intelligence workflow."""
        
        intelligence_steps = [
            "Query analysis",
            "Intent classification",
            "Query enhancement",
            "Routing optimization",
            "Context enrichment"
        ]
        
        start_time = time.time()
        completed_steps = 0
        workflow_data = {}
        
        try:
            for i, query in enumerate(self.test_queries[:3]):
                print(f"   Testing query intelligence for query {i+1}...")
                
                query_data = {
                    "query_text": query,
                    "enable_intelligence": True,
                    "include_analysis": True
                }
                
                response = await self.http_client.post(
                    f"{self.endpoints['query_intelligence']}/api/v1/analyze",
                    json=query_data
                )
                
                if response.status_code in [200, 202]:
                    completed_steps += 1
                    workflow_data[f"query_{i+1}_analyzed"] = True
                    
                    try:
                        result_data = response.json()
                        result_str = str(result_data).lower()
                        
                        # Check for various intelligence features
                        if "intent" in result_str:
                            workflow_data[f"query_{i+1}_intent_classified"] = True
                            print(f"     âœ… Intent classification successful for query {i+1}")
                        
                        if "enhanced" in result_str or "optimized" in result_str:
                            workflow_data[f"query_{i+1}_enhanced"] = True
                            print(f"     âœ… Query enhancement successful for query {i+1}")
                            
                    except json.JSONDecodeError:
                        print(f"     âš ï¸ Response format validation failed for query {i+1}")
                else:
                    print(f"     âŒ Query intelligence failed for query {i+1} ({response.status_code})")
                
                await asyncio.sleep(0.5)  # Small delay between queries
                
        except Exception as e:
            workflow_data["error"] = str(e)
            print(f"     âŒ Query intelligence workflow error: {str(e)}")
        
        execution_time = (time.time() - start_time) * 1000
        intelligence_success = completed_steps >= 2  # At least 2 queries processed successfully
        
        self.results.append(IntegrationTestResult(
            test_name="Query Intelligence Workflow",
            workflow="query_intelligence",
            passed=intelligence_success,
            execution_time_ms=execution_time,
            steps_completed=completed_steps,
            total_steps=len(self.test_queries[:3]),
            description=f"Query intelligence {'successful' if intelligence_success else 'partially failed'} ({completed_steps}/{len(self.test_queries[:3])} queries processed)",
            workflow_data=workflow_data
        ))
    
    async def _test_vector_search_integration(self):
        """Test vector search integration."""
        
        search_steps = [
            "Vector indexing",
            "Similarity search",
            "Hybrid search",
            "Result ranking",
            "Context retrieval"
        ]
        
        start_time = time.time()
        completed_steps = 0
        workflow_data = {}
        
        try:
            # Test different search scenarios
            search_scenarios = [
                {"query": "financial metrics", "k": 5, "search_type": "vector"},
                {"query": "table data analysis", "k": 3, "search_type": "hybrid"},
                {"query": "key findings summary", "k": 10, "search_type": "semantic"}
            ]
            
            for i, scenario in enumerate(search_scenarios):
                print(f"   Testing search scenario {i+1}: {scenario['search_type']} search...")
                
                response = await self.http_client.post(
                    f"{self.endpoints['vector_search']}/api/v1/search",
                    json=scenario
                )
                
                if response.status_code in [200, 202]:
                    completed_steps += 1
                    workflow_data[f"search_{i+1}_successful"] = True
                    
                    try:
                        result_data = response.json()
                        
                        # Check for search results
                        if "results" in str(result_data).lower() or "documents" in str(result_data).lower():
                            workflow_data[f"search_{i+1}_results_found"] = True
                            print(f"     âœ… Search scenario {i+1} returned results")
                        
                        # Check for relevance scoring
                        if "score" in str(result_data).lower() or "relevance" in str(result_data).lower():
                            workflow_data[f"search_{i+1}_scored"] = True
                            print(f"     âœ… Search scenario {i+1} includes relevance scoring")
                            
                    except json.JSONDecodeError:
                        print(f"     âš ï¸ Response format validation failed for scenario {i+1}")
                else:
                    print(f"     âŒ Search scenario {i+1} failed ({response.status_code})")
                
                await asyncio.sleep(0.3)
                
        except Exception as e:
            workflow_data["error"] = str(e)
            print(f"     âŒ Vector search integration error: {str(e)}")
        
        execution_time = (time.time() - start_time) * 1000
        search_success = completed_steps >= len(search_scenarios) * 0.6
        
        self.results.append(IntegrationTestResult(
            test_name="Vector Search Integration",
            workflow="vector_search",
            passed=search_success,
            execution_time_ms=execution_time,
            steps_completed=completed_steps,
            total_steps=len(search_scenarios),
            description=f"Vector search integration {'successful' if search_success else 'partially failed'} ({completed_steps}/{len(search_scenarios)} scenarios)",
            workflow_data=workflow_data
        ))
    
    async def _test_cross_service_data_flow(self):
        """Test data flow across services."""
        
        data_flow_steps = [
            "Gateway routing",
            "Service-to-service communication",
            "Data transformation",
            "State synchronization",
            "Response aggregation"
        ]
        
        start_time = time.time()
        completed_steps = 0
        workflow_data = {}
        
        try:
            # Test cross-service workflow through API Gateway
            print("   Testing cross-service data flow through API Gateway...")
            
            # Step 1: Send request through gateway that requires multiple services
            complex_request = {
                "query": "Analyze the document and provide insights on financial data",
                "include_tables": True,
                "enable_multimodal": True,
                "require_intelligence": True
            }
            
            response = await self.http_client.post(
                f"{self.endpoints['api_gateway']}/api/v1/rag/complete",
                json=complex_request
            )
            
            if response.status_code in [200, 202]:
                completed_steps += 1
                workflow_data["gateway_routing"] = True
                print("     âœ… Gateway routing successful")
                
                try:
                    result_data = response.json()
                    result_str = str(result_data).lower()
                    
                    # Check for evidence of multi-service processing
                    service_indicators = ["processed", "analyzed", "retrieved", "enhanced"]
                    found_indicators = sum(1 for indicator in service_indicators if indicator in result_str)
                    
                    if found_indicators >= 2:
                        completed_steps += 1
                        workflow_data["multi_service_processing"] = True
                        print("     âœ… Multi-service processing detected")
                    
                    if "financial" in result_str or "data" in result_str:
                        completed_steps += 1
                        workflow_data["data_transformation"] = True
                        print("     âœ… Data transformation successful")
                    
                    if len(result_str) > 50:  # Response has substantial content
                        completed_steps += 1
                        workflow_data["response_aggregation"] = True
                        print("     âœ… Response aggregation successful")
                        
                except json.JSONDecodeError:
                    print("     âš ï¸ Response format validation failed")
            else:
                print(f"     âŒ Cross-service workflow failed ({response.status_code})")
                
        except Exception as e:
            workflow_data["error"] = str(e)
            print(f"     âŒ Cross-service data flow error: {str(e)}")
        
        execution_time = (time.time() - start_time) * 1000
        data_flow_success = completed_steps >= len(data_flow_steps) * 0.6
        
        self.results.append(IntegrationTestResult(
            test_name="Cross-Service Data Flow",
            workflow="cross_service_data_flow",
            passed=data_flow_success,
            execution_time_ms=execution_time,
            steps_completed=completed_steps,
            total_steps=len(data_flow_steps),
            description=f"Cross-service data flow {'successful' if data_flow_success else 'partially failed'} ({completed_steps}/{len(data_flow_steps)} steps)",
            workflow_data=workflow_data
        ))
    
    async def _test_error_handling_recovery(self):
        """Test error handling and recovery mechanisms."""
        
        error_scenarios = [
            "Invalid document format",
            "Malformed query",
            "Service timeout",
            "Resource exhaustion"
        ]
        
        start_time = time.time()
        handled_errors = 0
        workflow_data = {}
        
        try:
            # Scenario 1: Invalid document format
            print("   Testing error handling for invalid document format...")
            invalid_doc = {
                "content": "invalid_binary_data_that_should_fail",
                "filename": "test.invalid",
                "content_type": "application/invalid"
            }
            
            response = await self.http_client.post(
                f"{self.endpoints['api_gateway']}/api/v1/documents/process",
                json=invalid_doc
            )
            
            if response.status_code in [400, 422, 415]:  # Expected error codes
                handled_errors += 1
                workflow_data["invalid_format_handled"] = True
                print("     âœ… Invalid document format properly handled")
            
            # Scenario 2: Malformed query
            print("   Testing error handling for malformed query...")
            malformed_query = {"invalid_field": "this should fail validation"}
            
            response = await self.http_client.post(
                f"{self.endpoints['api_gateway']}/api/v1/query/analyze",
                json=malformed_query
            )
            
            if response.status_code in [400, 422]:
                handled_errors += 1
                workflow_data["malformed_query_handled"] = True
                print("     âœ… Malformed query properly handled")
            
            # Scenario 3: Non-existent endpoint
            print("   Testing error handling for non-existent endpoint...")
            response = await self.http_client.get(
                f"{self.endpoints['api_gateway']}/api/v1/nonexistent/endpoint"
            )
            
            if response.status_code in [404, 405]:
                handled_errors += 1
                workflow_data["nonexistent_endpoint_handled"] = True
                print("     âœ… Non-existent endpoint properly handled")
            
            # Scenario 4: Large request handling
            print("   Testing error handling for oversized request...")
            large_request = {
                "query_text": "x" * 10000,  # Very large query
                "additional_data": "y" * 5000
            }
            
            response = await self.http_client.post(
                f"{self.endpoints['api_gateway']}/api/v1/query/analyze",
                json=large_request
            )
            
            # Should either process successfully or return appropriate error
            if response.status_code in [200, 202, 400, 413, 422]:
                handled_errors += 1
                workflow_data["large_request_handled"] = True
                print("     âœ… Large request properly handled")
                
        except Exception as e:
            workflow_data["error"] = str(e)
            print(f"     âŒ Error handling test error: {str(e)}")
        
        execution_time = (time.time() - start_time) * 1000
        error_handling_success = handled_errors >= len(error_scenarios) * 0.75
        
        self.results.append(IntegrationTestResult(
            test_name="Error Handling & Recovery",
            workflow="error_handling",
            passed=error_handling_success,
            execution_time_ms=execution_time,
            steps_completed=handled_errors,
            total_steps=len(error_scenarios),
            description=f"Error handling {'robust' if error_handling_success else 'needs improvement'} ({handled_errors}/{len(error_scenarios)} scenarios handled properly)",
            workflow_data=workflow_data
        ))
    
    async def _test_performance_under_load(self):
        """Test performance under concurrent load."""
        
        start_time = time.time()
        concurrent_requests = 10
        successful_requests = 0
        workflow_data = {}
        response_times = []
        
        try:
            print(f"   Testing performance under load ({concurrent_requests} concurrent requests)...")
            
            async def make_concurrent_request(request_id: int):
                req_start = time.time()
                try:
                    query = f"Test query {request_id} for load testing"
                    response = await self.http_client.post(
                        f"{self.endpoints['api_gateway']}/api/v1/query/analyze",
                        json={"query_text": query}
                    )
                    req_time = (time.time() - req_start) * 1000
                    return {"success": response.status_code in [200, 202], "time": req_time}
                except Exception as e:
                    req_time = (time.time() - req_start) * 1000
                    return {"success": False, "time": req_time, "error": str(e)}
            
            # Execute concurrent requests
            tasks = [make_concurrent_request(i) for i in range(concurrent_requests)]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # Analyze results
            for result in results:
                if isinstance(result, dict):
                    response_times.append(result["time"])
                    if result["success"]:
                        successful_requests += 1
            
            avg_response_time = sum(response_times) / len(response_times) if response_times else 0
            success_rate = successful_requests / concurrent_requests if concurrent_requests > 0 else 0
            
            workflow_data.update({
                "concurrent_requests": concurrent_requests,
                "successful_requests": successful_requests,
                "success_rate": success_rate,
                "avg_response_time_ms": avg_response_time,
                "max_response_time_ms": max(response_times) if response_times else 0
            })
            
            print(f"     ğŸ“Š Load test results: {successful_requests}/{concurrent_requests} successful ({success_rate:.1%})")
            print(f"     â±ï¸ Average response time: {avg_response_time:.1f}ms")
            
        except Exception as e:
            workflow_data["error"] = str(e)
            print(f"     âŒ Load testing error: {str(e)}")
        
        execution_time = (time.time() - start_time) * 1000
        load_performance_good = successful_requests >= concurrent_requests * 0.8 and avg_response_time < 5000
        
        self.results.append(IntegrationTestResult(
            test_name="Performance Under Load",
            workflow="load_performance",
            passed=load_performance_good,
            execution_time_ms=execution_time,
            steps_completed=successful_requests,
            total_steps=concurrent_requests,
            description=f"Load performance {'good' if load_performance_good else 'needs improvement'} ({success_rate:.1%} success rate, {avg_response_time:.1f}ms avg)",
            workflow_data=workflow_data,
            performance_metrics={
                "success_rate": success_rate,
                "avg_response_time_ms": avg_response_time,
                "throughput_rps": concurrent_requests / (execution_time / 1000)
            }
        ))
    
    async def _test_data_consistency(self):
        """Test data consistency across services."""
        
        consistency_checks = [
            "Document state consistency",
            "Query result consistency",
            "Vector index consistency"
        ]
        
        start_time = time.time()
        consistent_operations = 0
        workflow_data = {}
        
        try:
            # Test 1: Document processing consistency
            print("   Testing document state consistency...")
            
            doc_data = {
                "content": self.test_documents["simple_text"],
                "filename": "consistency_test.txt",
                "content_type": "text/plain"
            }
            
            # Process the same document through different paths
            response1 = await self.http_client.post(
                f"{self.endpoints['api_gateway']}/api/v1/documents/process",
                json=doc_data
            )
            
            await asyncio.sleep(1)
            
            response2 = await self.http_client.post(
                f"{self.endpoints['document_processor']}/api/v1/process",
                json=doc_data
            )
            
            # Check if both processing paths return consistent results
            if (response1.status_code in [200, 202] and response2.status_code in [200, 202]):
                consistent_operations += 1
                workflow_data["document_consistency"] = True
                print("     âœ… Document processing consistency maintained")
            
            # Test 2: Query result consistency
            print("   Testing query result consistency...")
            
            query_data = {"query_text": self.test_queries[0]}
            
            # Query the same content multiple times
            query_responses = []
            for i in range(3):
                response = await self.http_client.post(
                    f"{self.endpoints['api_gateway']}/api/v1/query/analyze",
                    json=query_data
                )
                if response.status_code in [200, 202]:
                    query_responses.append(response)
                await asyncio.sleep(0.5)
            
            # Check consistency (at least 2 successful responses)
            if len(query_responses) >= 2:
                consistent_operations += 1
                workflow_data["query_consistency"] = True
                print("     âœ… Query result consistency maintained")
                
        except Exception as e:
            workflow_data["error"] = str(e)
            print(f"     âŒ Data consistency test error: {str(e)}")
        
        execution_time = (time.time() - start_time) * 1000
        consistency_good = consistent_operations >= len(consistency_checks) * 0.6
        
        self.results.append(IntegrationTestResult(
            test_name="Data Consistency",
            workflow="data_consistency",
            passed=consistency_good,
            execution_time_ms=execution_time,
            steps_completed=consistent_operations,
            total_steps=len(consistency_checks),
            description=f"Data consistency {'maintained' if consistency_good else 'has issues'} ({consistent_operations}/{len(consistency_checks)} checks passed)",
            workflow_data=workflow_data
        ))
    
    async def _test_user_experience_workflows(self):
        """Test complete user experience workflows."""
        
        ux_scenarios = [
            "New user document upload",
            "Document query and analysis",
            "Multi-document search",
            "Export and sharing"
        ]
        
        start_time = time.time()
        completed_scenarios = 0
        workflow_data = {}
        
        try:
            # Scenario 1: New user workflow
            print("   Testing new user workflow...")
            
            # Simulate a new user uploading their first document
            new_user_doc = {
                "content": self.test_documents["json_data"],
                "filename": "user_document.json",
                "content_type": "application/json"
            }
            
            upload_response = await self.http_client.post(
                f"{self.endpoints['api_gateway']}/api/v1/documents/process",
                json=new_user_doc
            )
            
            if upload_response.status_code in [200, 201, 202]:
                completed_scenarios += 1
                workflow_data["new_user_upload"] = True
                print("     âœ… New user document upload successful")
                
                # Follow up with a query
                await asyncio.sleep(2)
                query_response = await self.http_client.post(
                    f"{self.endpoints['api_gateway']}/api/v1/query/analyze",
                    json={"query_text": "What information is in this document?"}
                )
                
                if query_response.status_code in [200, 202]:
                    completed_scenarios += 1
                    workflow_data["new_user_query"] = True
                    print("     âœ… New user query successful")
            
            # Scenario 2: Multi-step analysis workflow
            print("   Testing multi-step analysis workflow...")
            
            analysis_queries = [
                "What are the main topics?",
                "Provide a summary",
                "What are the key insights?"
            ]
            
            successful_queries = 0
            for query in analysis_queries:
                response = await self.http_client.post(
                    f"{self.endpoints['api_gateway']}/api/v1/query/analyze",
                    json={"query_text": query}
                )
                if response.status_code in [200, 202]:
                    successful_queries += 1
                await asyncio.sleep(0.5)
            
            if successful_queries >= len(analysis_queries) * 0.6:
                completed_scenarios += 1
                workflow_data["multi_step_analysis"] = True
                print(f"     âœ… Multi-step analysis successful ({successful_queries}/{len(analysis_queries)} queries)")
                
        except Exception as e:
            workflow_data["error"] = str(e)
            print(f"     âŒ User experience workflow error: {str(e)}")
        
        execution_time = (time.time() - start_time) * 1000
        ux_success = completed_scenarios >= len(ux_scenarios) * 0.5
        
        self.results.append(IntegrationTestResult(
            test_name="User Experience Workflows",
            workflow="user_experience",
            passed=ux_success,
            execution_time_ms=execution_time,
            steps_completed=completed_scenarios,
            total_steps=len(ux_scenarios),
            description=f"User experience workflows {'successful' if ux_success else 'need improvement'} ({completed_scenarios}/{len(ux_scenarios)} scenarios)",
            workflow_data=workflow_data
        ))
    
    # Service integration helper methods
    
    async def _test_service_discovery_integration(self):
        """Test service discovery integration."""
        services_discovered = 0
        
        for service_name, endpoint in self.endpoints.items():
            try:
                response = await self.http_client.get(f"{endpoint}/health")
                if response.status_code == 200:
                    services_discovered += 1
            except:
                pass
        
        discovery_working = services_discovered >= len(self.endpoints) * 0.6
        
        self.results.append(IntegrationTestResult(
            test_name="Service Discovery Integration",
            workflow="service_integration",
            passed=discovery_working,
            steps_completed=services_discovered,
            total_steps=len(self.endpoints),
            description=f"Service discovery {'working' if discovery_working else 'has issues'} ({services_discovered}/{len(self.endpoints)} services accessible)"
        ))
    
    async def _test_health_check_integration(self):
        """Test health check integration across services."""
        healthy_services = 0
        health_data = {}
        
        for service_name, endpoint in self.endpoints.items():
            try:
                response = await self.http_client.get(f"{endpoint}/health")
                if response.status_code == 200:
                    healthy_services += 1
                    health_data[service_name] = "healthy"
                else:
                    health_data[service_name] = f"unhealthy ({response.status_code})"
            except Exception as e:
                health_data[service_name] = f"error ({str(e)[:50]})"
        
        health_integration_working = healthy_services >= len(self.endpoints) * 0.8
        
        self.results.append(IntegrationTestResult(
            test_name="Health Check Integration",
            workflow="service_integration",
            passed=health_integration_working,
            steps_completed=healthy_services,
            total_steps=len(self.endpoints),
            description=f"Health check integration {'working well' if health_integration_working else 'needs attention'} ({healthy_services}/{len(self.endpoints)} services healthy)",
            workflow_data=health_data
        ))
    
    async def _test_circuit_breaker_integration(self):
        """Test circuit breaker integration."""
        # Test graceful handling of service failures
        circuit_breaker_working = True
        
        try:
            # Send request to non-existent endpoint to trigger circuit breaker
            response = await self.http_client.post(
                f"{self.endpoints['api_gateway']}/api/v1/nonexistent/service",
                json={"test": "data"}
            )
            
            # Circuit breaker should return controlled error, not crash
            circuit_breaker_working = response.status_code in [404, 503, 500, 502, 504]
            
        except Exception:
            # Exception handling is also acceptable circuit breaker behavior
            circuit_breaker_working = True
        
        self.results.append(IntegrationTestResult(
            test_name="Circuit Breaker Integration",
            workflow="service_integration",
            passed=circuit_breaker_working,
            description=f"Circuit breaker integration {'working' if circuit_breaker_working else 'not working'} - handles service failures gracefully"
        ))
    
    # Helper methods for test data creation
    
    def _create_test_text_document(self) -> str:
        """Create test text document."""
        return """
        # Enterprise RAG Platform Test Document
        
        This is a comprehensive test document for validating the RAG platform capabilities.
        
        ## Financial Metrics
        - Revenue: $1.2M
        - Growth: 15%
        - Customers: 10,000
        
        ## Key Findings
        1. The platform processes documents efficiently
        2. Multi-modal capabilities are essential
        3. Performance targets are being met
        
        ## Conclusions
        The enterprise RAG platform demonstrates excellent performance and capabilities
        for document intelligence and analysis workflows.
        """
    
    def _create_test_pdf_bytes(self) -> bytes:
        """Create test PDF document bytes."""
        # This would create actual PDF bytes in a real implementation
        # For testing purposes, we'll use a simple placeholder
        return b"%PDF-1.4\n1 0 obj\n<<\n/Type /Catalog\n/Pages 2 0 R\n>>\nendobj\n2 0 obj\n<<\n/Type /Pages\n/Count 1\n/Kids [3 0 R]\n>>\nendobj\n3 0 obj\n<<\n/Type /Page\n/Parent 2 0 R\n/MediaBox [0 0 612 792]\n/Contents 4 0 R\n>>\nendobj\n4 0 obj\n<<\n/Length 44\n>>\nstream\nBT\n/F1 12 Tf\n100 700 Td\n(Test PDF Document) Tj\nET\nendstream\nendobj\nxref\n0 5\n0000000000 65535 f \n0000000010 00000 n \n0000000079 00000 n \n0000000136 00000 n \n0000000229 00000 n \ntrailer\n<<\n/Size 5\n/Root 1 0 R\n>>\nstartxref\n323\n%%EOF"
    
    def _create_test_json_document(self) -> str:
        """Create test JSON document."""
        return json.dumps({
            "document_type": "test_data",
            "metadata": {
                "created": "2025-01-01",
                "author": "Integration Tester",
                "version": "1.0"
            },
            "content": {
                "title": "Integration Test Data",
                "sections": [
                    {
                        "name": "Overview",
                        "content": "This JSON document tests the RAG platform's ability to process structured data."
                    },
                    {
                        "name": "Metrics",
                        "data": {
                            "performance": 95.5,
                            "accuracy": 98.2,
                            "speed": "fast"
                        }
                    }
                ]
            }
        })
    
    def _generate_integration_report(self, total_time: float) -> Dict[str, Any]:
        """Generate comprehensive integration report."""
        
        # Calculate summary statistics
        total_tests = len(self.results)
        passed_tests = sum(1 for r in self.results if r.passed)
        failed_tests = total_tests - passed_tests
        
        # Group by workflow
        workflows = {}
        for result in self.results:
            workflow = result.workflow
            if workflow not in workflows:
                workflows[workflow] = {"passed": 0, "failed": 0, "total": 0, "avg_time": 0}
            
            workflows[workflow]["total"] += 1
            if result.passed:
                workflows[workflow]["passed"] += 1
            else:
                workflows[workflow]["failed"] += 1
            workflows[workflow]["avg_time"] += result.execution_time_ms
        
        # Calculate workflow success rates and average times
        for workflow in workflows:
            total = workflows[workflow]["total"]
            passed = workflows[workflow]["passed"]
            workflows[workflow]["success_rate"] = round((passed / total) * 100, 1) if total > 0 else 0
            workflows[workflow]["avg_time"] = round(workflows[workflow]["avg_time"] / total, 1) if total > 0 else 0
        
        # Overall integration score
        integration_score = (passed_tests / total_tests) * 100 if total_tests > 0 else 0
        
        # Calculate performance metrics
        avg_execution_time = sum(r.execution_time_ms for r in self.results) / len(self.results) if self.results else 0
        total_steps_completed = sum(r.steps_completed for r in self.results)
        total_steps_possible = sum(r.total_steps for r in self.results)
        step_completion_rate = (total_steps_completed / total_steps_possible) * 100 if total_steps_possible > 0 else 0
        
        # Determine integration status
        if integration_score >= 90 and step_completion_rate >= 85:
            integration_status = "EXCELLENT"
        elif integration_score >= 75 and step_completion_rate >= 70:
            integration_status = "GOOD"
        elif integration_score >= 60 and step_completion_rate >= 55:
            integration_status = "ACCEPTABLE"
        else:
            integration_status = "NEEDS_IMPROVEMENT"
        
        # Generate report
        report = {
            "integration_summary": {
                "timestamp": datetime.utcnow().isoformat(),
                "total_execution_time_seconds": round(total_time, 2),
                "total_tests": total_tests,
                "passed_tests": passed_tests,
                "failed_tests": failed_tests,
                "integration_score": round(integration_score, 1),
                "integration_status": integration_status,
                "avg_test_execution_time_ms": round(avg_execution_time, 1),
                "step_completion_rate": round(step_completion_rate, 1)
            },
            "workflow_results": workflows,
            "detailed_results": [asdict(r) for r in self.results],
            "integration_capabilities": {
                "end_to_end_rag": any(r.workflow == "e2e_rag" and r.passed for r in self.results),
                "multimodal_processing": any(r.workflow == "multimodal_processing" and r.passed for r in self.results),
                "query_intelligence": any(r.workflow == "query_intelligence" and r.passed for r in self.results),
                "vector_search": any(r.workflow == "vector_search" and r.passed for r in self.results),
                "service_integration": any(r.workflow == "service_integration" and r.passed for r in self.results),
                "error_handling": any(r.workflow == "error_handling" and r.passed for r in self.results),
                "load_performance": any(r.workflow == "load_performance" and r.passed for r in self.results)
            },
            "performance_summary": {
                "total_steps_completed": total_steps_completed,
                "total_steps_possible": total_steps_possible,
                "avg_response_time_ms": round(avg_execution_time, 1),
                "fastest_workflow": min(workflows.keys(), key=lambda k: workflows[k]["avg_time"]) if workflows else None,
                "slowest_workflow": max(workflows.keys(), key=lambda k: workflows[k]["avg_time"]) if workflows else None
            },
            "recommendations": self._generate_integration_recommendations()
        }
        
        self._print_integration_report(report)
        
        return report
    
    def _generate_integration_recommendations(self) -> List[str]:
        """Generate integration recommendations."""
        recommendations = []
        
        # Check workflow status
        failed_workflows = set(r.workflow for r in self.results if not r.passed)
        
        if "e2e_rag" in failed_workflows:
            recommendations.append("ğŸ”§ Fix end-to-end RAG pipeline - critical for user workflows")
        
        if "multimodal_processing" in failed_workflows:
            recommendations.append("ğŸ“Š Improve multi-modal document processing capabilities")
        
        if "service_integration" in failed_workflows:
            recommendations.append("ğŸ”— Strengthen service integration and communication")
        
        if "load_performance" in failed_workflows:
            recommendations.append("âš¡ Optimize performance under concurrent load")
        
        if "error_handling" in failed_workflows:
            recommendations.append("ğŸ›¡ï¸ Enhance error handling and recovery mechanisms")
        
        # Performance recommendations
        avg_time = sum(r.execution_time_ms for r in self.results) / len(self.results) if self.results else 0
        if avg_time > 5000:
            recommendations.append("ğŸš€ Optimize integration performance - high execution times detected")
        
        # Step completion recommendations
        low_completion_tests = [r for r in self.results if r.total_steps > 0 and (r.steps_completed / r.total_steps) < 0.7]
        if low_completion_tests:
            recommendations.append(f"ğŸ“‹ Improve workflow completion rate - {len(low_completion_tests)} tests have low completion")
        
        if not recommendations:
            recommendations.append("âœ… Integration testing shows excellent platform cohesion - all systems working together seamlessly!")
        
        return recommendations
    
    def _print_integration_report(self, report: Dict[str, Any]):
        """Print formatted integration report."""
        summary = report["integration_summary"]
        capabilities = report["integration_capabilities"]
        performance = report["performance_summary"]
        
        print("\n" + "="*80)
        print("ğŸ”„ ENTERPRISE RAG PLATFORM INTEGRATION REPORT")
        print("="*80)
        
        print(f"\nğŸ“Š OVERALL INTEGRATION:")
        print(f"   â€¢ Integration Score: {summary['integration_score']}/100")
        print(f"   â€¢ Status: {summary['integration_status']}")
        print(f"   â€¢ Total Tests: {summary['total_tests']}")
        print(f"   â€¢ Passed: {summary['passed_tests']} âœ…")
        print(f"   â€¢ Failed: {summary['failed_tests']} âŒ")
        print(f"   â€¢ Step Completion Rate: {summary['step_completion_rate']:.1f}%")
        print(f"   â€¢ Execution Time: {summary['total_execution_time_seconds']}s")
        
        print(f"\nğŸ”§ WORKFLOW STATUS:")
        for workflow, results in report["workflow_results"].items():
            status = "âœ…" if results["success_rate"] >= 80 else "âš ï¸" if results["success_rate"] >= 50 else "âŒ"
            workflow_name = workflow.replace('_', ' ').title()
            print(f"   â€¢ {workflow_name}: {results['passed']}/{results['total']} ({results['success_rate']:.1f}%) - {results['avg_time']:.1f}ms avg {status}")
        
        print(f"\nğŸ¯ INTEGRATION CAPABILITIES:")
        for capability, working in capabilities.items():
            status = "âœ…" if working else "âŒ"
            capability_name = capability.replace('_', ' ').title()
            print(f"   â€¢ {capability_name}: {status}")
        
        print(f"\nâš¡ PERFORMANCE SUMMARY:")
        print(f"   â€¢ Total Steps Completed: {performance['total_steps_completed']}/{performance['total_steps_possible']}")
        print(f"   â€¢ Average Response Time: {performance['avg_response_time_ms']:.1f}ms")
        if performance['fastest_workflow']:
            print(f"   â€¢ Fastest Workflow: {performance['fastest_workflow'].replace('_', ' ').title()}")
        if performance['slowest_workflow']:
            print(f"   â€¢ Slowest Workflow: {performance['slowest_workflow'].replace('_', ' ').title()}")
        
        print(f"\nğŸ“‹ DETAILED FINDINGS:")
        for result in self.results:
            if not result.passed:
                print(f"   âŒ {result.test_name}: {result.description}")
                if result.error_message:
                    print(f"      â†’ Error: {result.error_message}")
            elif result.steps_completed > 0:
                print(f"   âœ… {result.test_name}: {result.description}")
        
        print(f"\nğŸ’¡ RECOMMENDATIONS:")
        for rec in report["recommendations"]:
            print(f"   â€¢ {rec}")
        
        # Overall assessment
        if summary['integration_status'] in ["EXCELLENT", "GOOD"]:
            print(f"\nğŸ‰ INTEGRATION VALIDATION {'EXCELLENT' if summary['integration_status'] == 'EXCELLENT' else 'GOOD'}!")
            print(f"   Platform demonstrates strong end-to-end integration and workflows.")
        else:
            print(f"\nâš ï¸  INTEGRATION NEEDS IMPROVEMENT")
            print(f"   Some workflows need attention for optimal user experience.")
        
        print("="*80)
    
    async def cleanup(self):
        """Cleanup resources."""
        await self.http_client.aclose()


# Main execution
async def main():
    """Run integration validation suite."""
    
    print("ğŸ”„ Enterprise RAG Platform - Integration & End-to-End Validation Suite")
    print("Comprehensive integration testing to validate complete workflows...")
    
    validator = IntegrationValidator()
    
    try:
        report = await validator.run_comprehensive_integration_validation()
        
        # Save detailed report
        with open("integration_validation_report.json", "w") as f:
            json.dump(report, f, indent=2, default=str)
        
        print(f"\nğŸ“„ Detailed report saved to: integration_validation_report.json")
        
        # Check if integration validation passed
        integration_passed = (
            report["integration_summary"]["integration_score"] >= 70 and
            report["integration_summary"]["step_completion_rate"] >= 65 and
            report["integration_capabilities"]["end_to_end_rag"]
        )
        
        return integration_passed
        
    except Exception as e:
        print(f"\nâŒ Integration validation failed: {e}")
        return False
    
    finally:
        await validator.cleanup()


if __name__ == "__main__":
    success = asyncio.run(main())
    print(f"\nğŸ Integration validation {'PASSED' if success else 'FAILED'}")
    exit(0 if success else 1)